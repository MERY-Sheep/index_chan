use anyhow::{Context, Result};
use candle_core::{Device, IndexOp, Tensor};
use candle_nn::VarBuilder;
use candle_transformers::models::qwen2::{Config as Qwen2Config, ModelForCausalLM as Qwen2Model};
use hf_hub::{api::sync::Api, Repo, RepoType};
use tokenizers::Tokenizer;

use super::config::LLMConfig;

#[allow(dead_code)]
pub struct LLMModel {
    model: Qwen2Model,
    tokenizer: Tokenizer,
    device: Device,
    config: LLMConfig,
}

impl LLMModel {
    pub fn new(config: LLMConfig) -> Result<Self> {
        println!("ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {}", config.model_name);

        let device = Device::Cpu;

        // Check if local model directory exists
        let local_model_dir = std::path::PathBuf::from("models");
        let tokenizer_path = if local_model_dir.join("tokenizer.json").exists() {
            local_model_dir.join("tokenizer.json")
        } else {
            local_model_dir.join("tokenizer_config.json")
        };

        let use_local = local_model_dir.exists()
            && local_model_dir.join("config.json").exists()
            && tokenizer_path.exists()
            && local_model_dir.join("model.safetensors").exists();

        let (config_file, tokenizer_file, model_file) = if use_local {
            println!("  ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: ./models/");
            (
                local_model_dir.join("config.json"),
                tokenizer_path,
                local_model_dir.join("model.safetensors"),
            )
        } else {
            println!("  HuggingFace Hubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...");
            println!("  ğŸ’¡ åˆå›å®Ÿè¡Œæ™‚ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™");

            // Download model from HuggingFace Hub using model() method
            let api = Api::new().context(
                "HuggingFace APIã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ\n\
                 ğŸ’¡ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:\n\
                    1. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„\n\
                    2. ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«å†…ã®å ´åˆã¯ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„\n\
                    3. HuggingFace HubãŒãƒ€ã‚¦ãƒ³ã—ã¦ã„ã‚‹å ´åˆã¯å¾Œã§å†è©¦è¡Œã—ã¦ãã ã•ã„"
            )?;
            let model_repo = api.model(config.model_name.clone());

            println!("    - config.json");
            let config_file = model_repo
                .get("config.json")
                .context("config.jsonã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\n\
                         ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ")?;

            println!("    - tokenizer.json");
            let tokenizer_file = model_repo
                .get("tokenizer.json")
                .context("tokenizer.jsonã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")?;

            println!("    - model.safetensors (~3GB)");
            let model_file = model_repo
                .get("model.safetensors")
                .context("model.safetensorsã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\n\
                         ğŸ’¡ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¤§ãã„ã§ã™(~3GB)ã€‚ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:\n\
                            - å®‰å®šã—ãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š\n\
                            - ååˆ†ãªãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡\n\
                            - æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™(5-10åˆ†ç¨‹åº¦)")?;

            (config_file, tokenizer_file, model_file)
        };

        println!("  ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...");
        let tokenizer = Tokenizer::from_file(tokenizer_file)
            .map_err(|e| anyhow::anyhow!("ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {}", e))?;

        println!("  ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...");
        let model_config: Qwen2Config = serde_json::from_reader(std::fs::File::open(config_file)?)
            .context("ãƒ¢ãƒ‡ãƒ«è¨­å®šã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ")?;

        println!("  ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...");
        println!("  ğŸ’¡ ç´„3GBã®RAMãŒå¿…è¦ã§ã™");
        // Use F32 for better compatibility
        let dtype = candle_core::DType::F32;
        let vb = unsafe { 
            VarBuilder::from_mmaped_safetensors(&[model_file], dtype, &device)
                .context("ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\n\
                         ğŸ’¡ è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :\n\
                            - ãƒ¡ãƒ¢ãƒªä¸è¶³(ç´„3GBã®RAMãŒå¿…è¦)\n\
                            - ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ç ´æ(~/.cache/huggingfaceã‚’å‰Šé™¤ã—ã¦ã¿ã¦ãã ã•ã„)\n\
                            - äº’æ›æ€§ã®ãªã„ãƒ¢ãƒ‡ãƒ«å½¢å¼")?
        };

        let model = Qwen2Model::new(&model_config, vb)
            .context("ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")?;

        println!("âœ… ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†");

        Ok(Self {
            model,
            tokenizer,
            device,
            config,
        })
    }

    pub fn generate(&mut self, prompt: &str) -> Result<String> {
        // Format prompt for Qwen2.5-Coder-Instruct
        let formatted_prompt = format!(
            "<|im_start|>system\nYou are a helpful code analysis assistant.<|im_end|>\n<|im_start|>user\n{}<|im_end|>\n<|im_start|>assistant\n",
            prompt
        );

        // Tokenize input
        let encoding = self
            .tokenizer
            .encode(formatted_prompt.as_str(), true)
            .map_err(|e| anyhow::anyhow!("ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«å¤±æ•—: {}", e))?;

        let tokens = encoding.get_ids();
        println!("  å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {}", tokens.len());

        if tokens.len() > 2000 {
            anyhow::bail!(
                "å…¥åŠ›ãŒé•·ã™ãã¾ã™: {}ãƒˆãƒ¼ã‚¯ãƒ³ (æœ€å¤§2000)\n\
                 ğŸ’¡ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸›ã‚‰ã™ã‹ã€åˆ†æã‚’åˆ†å‰²ã—ã¦ãã ã•ã„",
                tokens.len()
            );
        }

        // Generate tokens
        let mut generated_tokens = tokens.to_vec();
        let max_new_tokens = self.config.max_tokens.min(50); // Limit for testing

        for step in 0..max_new_tokens {
            // For first step, use all tokens; for subsequent steps, use only the last token
            let input_tokens = if step == 0 {
                &generated_tokens[..]
            } else {
                &generated_tokens[generated_tokens.len() - 1..]
            };

            let input = Tensor::new(input_tokens, &self.device)?.unsqueeze(0)?;

            let start_pos = if step == 0 {
                0
            } else {
                generated_tokens.len() - 1
            };

            // Forward pass (ModelForCausalLM already returns logits for the last position)
            let logits = self.model.forward(&input, start_pos)?;
            let last_logits = logits.squeeze(0)?.squeeze(0)?; // shape: [vocab_size]

            // Sample next token (greedy for now)
            let next_token = last_logits.argmax(0)?.to_scalar::<u32>()?;

            // Check for EOS token
            let eos_tokens = vec![
                self.tokenizer.token_to_id("<|endoftext|>"),
                self.tokenizer.token_to_id("<|im_end|>"),
                self.tokenizer.token_to_id("</s>"),
            ];

            if eos_tokens.iter().any(|&t| t == Some(next_token)) {
                println!("  EOSæ¤œå‡º (ã‚¹ãƒ†ãƒƒãƒ— {})", step + 1);
                break;
            }

            generated_tokens.push(next_token);

            if (step + 1) % 10 == 0 {
                println!("  ç”Ÿæˆä¸­... {}ãƒˆãƒ¼ã‚¯ãƒ³", step + 1);
            }
        }

        println!(
            "  ç”Ÿæˆå®Œäº†: {}ãƒˆãƒ¼ã‚¯ãƒ³",
            generated_tokens.len() - tokens.len()
        );

        // Decode output
        let output = self
            .tokenizer
            .decode(&generated_tokens[tokens.len()..], true)
            .map_err(|e| anyhow::anyhow!("ãƒ‡ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—: {}", e))?;

        Ok(output)
    }
}
